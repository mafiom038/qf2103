{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Strategy Selection for All Stocks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.152961Z",
     "start_time": "2025-04-18T05:59:03.148712Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "from anyio.to_interpreter import run_sync\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.187937Z",
     "start_time": "2025-04-18T05:59:03.184497Z"
    }
   },
   "source": [
    "DATA_PATH = 'Data'\n",
    "PREDICTION_FOLDER = 'Predictions'\n",
    "\n",
    "RSI_START_STR = '2024-01-17'\n",
    "RSI_TEST_START_STR = '2024-03-01'\n",
    "RSI_END_STR = '2025-01-16'\n",
    "\n",
    "LNR_RF_TRAIN_START_STR = '2024-01-17'\n",
    "LNR_RF_TRAIN_END_STR = '2024-02-28'\n",
    "LNR_RF_TEST_START_STR = '2024-02-29'\n",
    "LNR_RF_TEST_END_STR = '2025-01-16'\n",
    "\n",
    "LGR_TRAIN_START_STR = '2021-03-01'\n",
    "LGR_TRAIN_END_STR = '2024-02-29'\n",
    "LGR_TEST_START_STR = '2024-03-01'\n",
    "LGR_TEST_END_STR = '2025-01-16'\n",
    "\n",
    "MLP_TRAIN_START_STR = '2021-01-01'\n",
    "MLP_TRAIN_END_STR = '2024-02-29'\n",
    "MLP_TEST_START_STR = '2024-03-01'\n",
    "MLP_TEST_END_STR = '2025-01-16'"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 1: RSI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.203277Z",
     "start_time": "2025-04-18T05:59:03.195636Z"
    }
   },
   "source": [
    "# Function to compute RSI for a given series and window\n",
    "def compute_RSI(series, window):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=window, min_periods=window).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=window).mean()\n",
    "    RS = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero\n",
    "    RSI = 100 - (100 / (1 + RS))\n",
    "    return RSI\n",
    "\n",
    "# Function to build the RSI strategy with long and short positions\n",
    "def rsi_strategy(df, window, oversold=30, overbought=80):\n",
    "    df['RSI'] = compute_RSI(df['Close'], window)\n",
    "\n",
    "    # Generate trading signals: +1 for long, -0.5 for short\n",
    "    df['Signal'] = 0\n",
    "    df['Signal'] = df['Signal'].astype(float)\n",
    "    df.loc[df['RSI'] < oversold, 'Signal'] = 1  # Long\n",
    "    df.loc[df['RSI'] > overbought, 'Signal'] = -0.5  # Short (Modified from -1 to -0.5)\n",
    "\n",
    "    # Carry forward the last signal until a new signal appears\n",
    "    df['pos'] = df['Signal'].replace(to_replace=0, method='ffill').fillna(0)\n",
    "\n",
    "    # Calculate daily returns and strategy returns\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Strategy_Returns'] = df['pos'].shift(1) * df['Returns']\n",
    "    df['Strategy_Returns'].fillna(0, inplace=True)\n",
    "\n",
    "    # Compute cumulative returns\n",
    "    test_df = df.loc[RSI_TEST_START_STR:RSI_END_STR]\n",
    "    test_df['Cumulative_Return_Strategy'] = (1 + test_df['Strategy_Returns']).cumprod()\n",
    "    total_return = test_df['Cumulative_Return_Strategy'].iloc[-1]\n",
    "\n",
    "    return total_return, test_df\n",
    "\n",
    "# Function to perform grid search over RSI windows\n",
    "def find_best_rsi_window(df, window_range):\n",
    "    results = {}\n",
    "    for window in window_range:\n",
    "        total_return, _ = rsi_strategy(df, window)\n",
    "        results[window] = total_return\n",
    "    best_window = max(results, key=results.get)\n",
    "    _, best_strategy_df = rsi_strategy(df, best_window)\n",
    "    return best_window, results, best_strategy_df\n",
    "\n",
    "def run_strategy_rsi(data, ticker):\n",
    "    # Use your existing grid search functions\n",
    "    data = data.loc[RSI_START_STR:RSI_END_STR].copy()\n",
    "\n",
    "    window_range = range(5, 61, 3)\n",
    "    best_window, results, best_strategy_df = find_best_rsi_window(data, window_range)\n",
    "    # Calculate total (net) return: subtract 1 to get a percentage gain/loss\n",
    "    total_return = best_strategy_df['Cumulative_Return_Strategy'].iloc[-1] - 1\n",
    "    return best_strategy_df"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 2: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.220064Z",
     "start_time": "2025-04-18T05:59:03.209297Z"
    }
   },
   "source": [
    "def create_lags(df: pd.DataFrame, col: str, n_lags: int):\n",
    "    '''\n",
    "    Generate lagged features for a specified column\n",
    "    '''\n",
    "    lagged_cols = []\n",
    "\n",
    "    for lag in range(1, 1 + n_lags):\n",
    "        df[f'Lag_{lag}_{col}'] = df[col].shift(lag)\n",
    "        lagged_cols.append(f'Lag_{lag}_{col}')\n",
    "\n",
    "    return lagged_cols\n",
    "\n",
    "def scale_columns(df, window:int=10, inplace:bool = False):\n",
    "    # Scale columns with large/values\n",
    "    cols_to_scale = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # Apply rolling standard scaling for each column\n",
    "    for col in cols_to_scale:\n",
    "        rolling_mean = df[col].rolling(window=window).mean().shift(1)\n",
    "        rolling_std = df[col].rolling(window=window).std().shift(1)\n",
    "        df[f'{col}_scaled'] = (df[col] - rolling_mean) / rolling_std\n",
    "\n",
    "    for col in cols_to_scale:\n",
    "        df[col] = df[f'{col}_scaled']\n",
    "        df.drop(columns=[f'{col}_scaled'], inplace=True)\n",
    "        df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    create_lags(df, 'Returns', 5)\n",
    "    df = df.dropna()\n",
    "    df['Directions'] = np.sign(df['Returns']).astype(int)\n",
    "    \n",
    "    # Here the target variables are the next day's values\n",
    "    df['Target'] = df['Directions'].shift(-1)\n",
    "    df['Target_Returns'] = df['Returns'].shift(-1)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = scale_columns(df, 10)\n",
    "    return df\n",
    "\n",
    "def stock_train_test_split_lnr_rf(df):\n",
    "    feature_cols = ['Close', 'High', 'Low', 'Open', 'Volume', 'Returns', 'Lag_1_Returns', 'Lag_2_Returns',\n",
    "           'Lag_3_Returns', 'Lag_4_Returns', 'Lag_5_Returns']\n",
    "    train_data = df.loc[LNR_RF_TRAIN_START_STR:LNR_RF_TRAIN_END_STR].copy()\n",
    "    test_data = df.loc[LNR_RF_TEST_START_STR:LNR_RF_TEST_END_STR].copy()\n",
    "    return (train_data[feature_cols], test_data[feature_cols], train_data[['Target', 'Target_Returns']] , test_data[['Target', 'Target_Returns']])\n",
    "\n",
    "def fit_and_predict_models(stock, X_train, X_test, y_train, y_test, save_to=None, flag='linreg'):\n",
    "    df_result = pd.DataFrame() if not save_to else save_to\n",
    "    if flag == 'linreg':\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train['Target'])\n",
    "        y_test[f'pos'] = model.predict(X_test)\n",
    "        y_test[f'pos'] = np.sign(y_test[f'pos'])\n",
    "        y_test[f'pos'] = np.where(y_test[f'pos'] < -0.5, -0.5, y_test[f'pos'])\n",
    "        df_result[f'pos'] = y_test['pos']\n",
    "        df_result[f'ret'] = y_test[f'pos'] * y_test['Target_Returns']\n",
    "        df_result[f'Cumulative_Return_Strategy'] = np.exp(df_result[f'ret'].cumsum())\n",
    "    \n",
    "    if flag == 'rf':\n",
    "        seed = 12345\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "        model.fit(X_train, y_train['Target'])\n",
    "        y_test[f'pos'] = model.predict(X_test)\n",
    "        y_test[f'pos'] = np.sign(y_test[f'pos'])\n",
    "        y_test[f'pos'] = np.where(y_test[f'pos'] < -0.5, -0.5, y_test[f'pos'])\n",
    "        df_result[f'pos'] = y_test['pos']\n",
    "        df_result[f'ret'] = y_test[f'pos'] * y_test['Target_Returns']\n",
    "        df_result[f'Cumulative_Return_Strategy'] = np.exp(df_result[f'ret'].cumsum())\n",
    "\n",
    "    # Compute benchmark cumulative returns\n",
    "    benchmark_ret = y_test['Target_Returns']\n",
    "    df_result['cum_ret_benchmark'] = np.exp(benchmark_ret.cumsum())\n",
    "    \n",
    "    # As the target variables that we defined above are the next day's values, we need to revert it back\n",
    "    df_result.index = y_test.index\n",
    "    return df_result\n",
    "\n",
    "def run_strategy_linreg(data, stock):\n",
    "    data = data.loc[LNR_RF_TRAIN_START_STR:LNR_RF_TEST_END_STR].copy()\n",
    "    data = preprocess_data(data)\n",
    "    if 'Stock Splits' in data.columns:\n",
    "        data = data.drop(columns=['Stock Splits'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = stock_train_test_split_lnr_rf(data)\n",
    "    df_result = fit_and_predict_models(stock=stock, X_train=X_train, X_test=X_test, \n",
    "                                       y_train=y_train, y_test=y_test, flag='linreg')\n",
    "    \n",
    "    # Add close price data for benchmark comparison\n",
    "    close_price = data.loc[df_result.index, 'Close']\n",
    "    df_result['Close'] = close_price\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def run_strategy_rf(data, stock):\n",
    "    data = data.loc[LNR_RF_TRAIN_START_STR:LNR_RF_TEST_END_STR].copy()\n",
    "    data = preprocess_data(data)\n",
    "    if 'Stock Splits' in data.columns:\n",
    "        data = data.drop(columns=['Stock Splits'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = stock_train_test_split_lnr_rf(data)\n",
    "    df_result = fit_and_predict_models(stock=stock, X_train=X_train, X_test=X_test,\n",
    "                                       y_train=y_train, y_test=y_test, flag='rf')\n",
    "\n",
    "    # Add close price data for benchmark comparison\n",
    "    close_price = data.loc[df_result.index, 'Close']\n",
    "    df_result['Close'] = close_price\n",
    "\n",
    "    return df_result"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 4: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.244434Z",
     "start_time": "2025-04-18T05:59:03.229980Z"
    }
   },
   "source": [
    "def calculate_moving_averages(data):\n",
    "    sma10 = data['Close'].shift(1).rolling(window = 10).mean()\n",
    "    sma50 = data['Close'].shift(1).rolling(window = 50).mean()\n",
    "    ema20 = data['Close'].shift(1).ewm(span = 20, adjust = False).mean()\n",
    "    return (sma10, sma50, ema20)\n",
    "\n",
    "def calculate_bb(data):\n",
    "    rolling_mean = data['Close'].shift(1).rolling(window = 20).mean()\n",
    "    rolling_std = data['Close'].shift(1).rolling(window = 20).std()\n",
    "    boll_upper = rolling_mean + (2 * rolling_std)\n",
    "    boll_lower = rolling_mean - (2 * rolling_std)\n",
    "    return (boll_upper, boll_lower)\n",
    "\n",
    "def calculate_macd(data):\n",
    "    macd = data['Close'].shift(1).ewm(span = 12, adjust = False).mean() - data['Close'].shift(1).ewm(span = 26, adjust = False).mean()\n",
    "    macd_signal = macd.ewm(span = 9, adjust = False).mean()\n",
    "    return (macd, macd_signal)\n",
    "\n",
    "def momentum_oscillator(data):\n",
    "    stoch_k = ((data['Close'].shift(1) - data['Low'].shift(1).rolling(window = 14).min()) /\n",
    "                   (data['High'].shift(1).rolling(window = 14).max() - data['Low'].shift(1).rolling(window = 14).min())) * 100\n",
    "\n",
    "    williams_R = ((data['High'].shift(1).rolling(window = 14).max() - data['Close'].shift(1)) /\n",
    "                       (data['High'].shift(1).rolling(window = 14).max() - data['Low'].shift(1).rolling(window = 14).min())) * -100\n",
    "    return (stoch_k, williams_R)\n",
    "\n",
    "def volume_indicators(data):\n",
    "    # On-balance Volume (OBV)\n",
    "    obv = (np.sign(data['Close'].shift(1).diff()) * data['Volume'].shift(1)).fillna(0).cumsum()\n",
    "\n",
    "    # VWAP (Volume Weighted Average Price)\n",
    "    vwap = (data['Close'].shift(1) * data['Volume'].shift(1)).cumsum() / data['Volume'].shift(1).cumsum()\n",
    "    return (obv, vwap)\n",
    "\n",
    "def directional_movement(data):\n",
    "    dm_plus = np.where((data['High'].shift(1) - data['High'].shift(2)) > (data['Low'].shift(2) - data['Low'].shift(1)), \n",
    "                           np.maximum(data['High'].shift(1) - data['High'].shift(2), 0), 0)\n",
    "    dm_minus = np.where((data['Low'].shift(3) - data['Low'].shift(1)) > (data['High'].shift(1) - data['High'].shift(2)), \n",
    "                            np.maximum(data['Low'].shift(2) - data['Low'].shift(1), 0), 0)\n",
    "    return (dm_plus, dm_minus)\n",
    "\n",
    "def feature_selection(data, ticker):\n",
    "    stock_config = {\n",
    "        'NKE': {'top_k': 7, 'random_state': 42},\n",
    "    }\n",
    "    default_config = {'top_k': 9, 'random_state': 2}\n",
    "    \n",
    "    config = stock_config.get(ticker, default_config)\n",
    "\n",
    "    features = data.columns[7:]\n",
    "    train_data = data[LGR_TRAIN_START_STR:LGR_TRAIN_END_STR].copy()\n",
    "    X = train_data[features]\n",
    "    y = train_data['direction']\n",
    "\n",
    "    # Mutual Information\n",
    "    mi_scores = mutual_info_classif(X, y, discrete_features=False, random_state=config['random_state'])\n",
    "    mi_selected = pd.Series(mi_scores, index=features).nlargest(config['top_k']).index.tolist()\n",
    "\n",
    "    # RFE\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=config['random_state'])\n",
    "    rfe = RFE(rf, n_features_to_select=config['top_k'])\n",
    "    rfe.fit(X, y)\n",
    "    rfe_selected = X.columns[rfe.support_].tolist()\n",
    "\n",
    "    return list(set(mi_selected) | set(rfe_selected))\n",
    "\n",
    "def run_strategy_logreg(data, ticker):\n",
    "    train_data = data.loc[LGR_TRAIN_START_STR:LGR_TRAIN_END_STR].copy()\n",
    "    test_data = data.loc[LGR_TEST_START_STR:LGR_TEST_END_STR].copy()\n",
    "    df2 = train_data[['Close', 'High', 'Low', 'Open', 'Volume']].copy()\n",
    "    df3 = test_data[['Close', 'High', 'Low', 'Open', 'Volume']].copy()\n",
    "    \n",
    "    # Combine all data\n",
    "    df = pd.concat((df2, df3))\n",
    "    df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df['direction'] = (df['Returns'] > 0).astype(int)\n",
    "    df['direction'] = np.where(df['direction'] == 0, -1, df['direction'])\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df['SMA_10'], df['SMA_50'], df['EMA_20'] = calculate_moving_averages(df)\n",
    "    df['boll_upper'], df['boll_lower'] = calculate_bb(df)\n",
    "    df['MACD'], df['MACD_signal'] = calculate_macd(df)\n",
    "    df['stoch_k'], df['williams_R'] = momentum_oscillator(df)\n",
    "    df['OBV'], df['VWAP'] = volume_indicators(df)\n",
    "    \n",
    "    for lag in range(1, 6):\n",
    "        df[f'lag_{lag}'] = df['Returns'].shift(lag)\n",
    "        \n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Add stock name to DataFrame for feature selection\n",
    "    df.attrs['stock'] = ticker\n",
    "    final_selected_features = feature_selection(df, ticker)\n",
    "    scaler = StandardScaler()\n",
    "    df[final_selected_features] = scaler.fit_transform(df[final_selected_features])\n",
    "    \n",
    "    train_data_new = df.loc[LGR_TRAIN_START_STR:LGR_TRAIN_END_STR].copy()\n",
    "    test_data_new = df.loc[LGR_TEST_START_STR:LGR_TEST_END_STR].copy()\n",
    "    \n",
    "    X_train, y_train = train_data_new[final_selected_features], train_data_new['direction']\n",
    "    X_test, y_test = test_data_new[final_selected_features], test_data_new['direction']\n",
    "    \n",
    "    # Train Models\n",
    "    if ticker == 'CAT':\n",
    "        model = LogisticRegression(solver='lbfgs', C=0.7, random_state=12345)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "    elif ticker == 'NKE': #sag 0.5\n",
    "        model = LogisticRegression(solver='liblinear', C=0.6, random_state=12345)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "    else:\n",
    "        model = LogisticRegression(solver='lbfgs', C=0.6, random_state=12345)\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    test_data_new['pos'] = np.where(predictions < 0, -0.5, 1)\n",
    "    test_data_new['strategy_returns'] = test_data_new['pos'] * test_data_new['Returns']\n",
    "    test_data_new['Cumulative_Return_Strategy'] = test_data_new['strategy_returns'].cumsum().apply(np.exp)\n",
    "    \n",
    "    # Calculate cumulative (gross) return (subtract 1 if you prefer net return)\n",
    "    cumulative_return = np.exp(test_data_new['strategy_returns'].sum()) - 1\n",
    "    return test_data_new"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 5: MLP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.260506Z",
     "start_time": "2025-04-18T05:59:03.252184Z"
    }
   },
   "source": [
    "def select_feature(X_train, y_train):\n",
    "    # 1. Mutual Information (MI)\n",
    "    mi_scores = mutual_info_classif(X_train, y_train, discrete_features=False,random_state=42)\n",
    "    mi_selected = pd.Series(mi_scores, index=X_train.columns).nlargest(3).index.tolist()\n",
    "    \n",
    "    # 2. Recursive Feature Elimination (RFE) with RandomForest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rfe = RFE(rf, n_features_to_select=3)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    rfe_selected = X_train.columns[rfe.support_].tolist()\n",
    "    \n",
    "    # Select Top Features (Union of All Methods)\n",
    "    selected_features = list(set(mi_selected + rfe_selected))\n",
    "    return selected_features\n",
    "\n",
    "def run_strategy_mlp(data, ticker):\n",
    "    data_train = data.loc[MLP_TRAIN_START_STR:MLP_TRAIN_END_STR].copy()\n",
    "    data_test = data.loc[MLP_TEST_START_STR:MLP_TEST_END_STR].copy()\n",
    "\n",
    "    df2 = data_train[['Close', 'High', 'Open', 'Low', 'Volume']].copy()\n",
    "    df3 = data_test[['Close', 'High', 'Open', 'Low', 'Volume']].copy()\n",
    "    \n",
    "    df = pd.concat([df2, df3])\n",
    "    df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    df['direction'] = np.where(df['Returns'] > 0, 1, 0)\n",
    "    ma = calculate_moving_averages(df)\n",
    "    df['SMA_10'] = ma[0]\n",
    "    df['EMA_20'] = ma[2]\n",
    "    df['boll_upper'], df['boll_lower'] = calculate_bb(df)\n",
    "    df['MACD'], df['MACD_signal'] = calculate_macd(df)\n",
    "    df['OBV'] = volume_indicators(df)[0]\n",
    "    df['williams_R'] = momentum_oscillator(df)[1]\n",
    "    df['DM_plus'], df['DM_minus'] = directional_movement(df)\n",
    "    df['Lag_Close'] = df['Close'].shift(1)\n",
    "    df['Lag_Volume'] = df['Volume'].shift(1)\n",
    "    for lag in range(1, 6):\n",
    "        df[f'lag_{lag}'] = df['Returns'].shift(lag)\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    all_features = df.columns[7:]\n",
    "    scaler = StandardScaler()\n",
    "    df[all_features] = scaler.fit_transform(df[all_features])\n",
    "    \n",
    "    df_train = df[MLP_TRAIN_START_STR:MLP_TRAIN_END_STR].copy()\n",
    "    df_test = df[MLP_TEST_START_STR:MLP_TEST_END_STR].copy()\n",
    "    \n",
    "    all_predictions = []\n",
    "    current_start = pd.to_datetime(MLP_TEST_START_STR)\n",
    "    \n",
    "    while current_start <= df_test.index[-1]:\n",
    "        current_end = (current_start + pd.DateOffset(months=1)).replace(day=1)\n",
    "        current_test = df_test[(df_test.index >= current_start) & (df_test.index < current_end)]\n",
    "        if current_test.empty:\n",
    "            break\n",
    "        X_train = df_train[all_features]\n",
    "        y_train = df_train['direction']\n",
    "        selected_features = select_feature(X_train, y_train)\n",
    "        model = MLPClassifier(hidden_layer_sizes=(100, 70, 70), activation='logistic',\n",
    "                            max_iter=1000, random_state=12345)\n",
    "        model.fit(X_train[selected_features], y_train)\n",
    "        X_test = current_test[selected_features]\n",
    "        preds = model.predict(X_test)\n",
    "        pos = np.where(preds == 0, -0.5, 1)\n",
    "        all_predictions.extend(pos)\n",
    "        df_train = pd.concat([df_train, current_test])\n",
    "        current_start = current_end\n",
    "        \n",
    "    # Ensure df_test and predictions have the same length\n",
    "    df_test = df_test.iloc[:len(all_predictions)].copy()\n",
    "    df_test['pos'] = all_predictions\n",
    "    df_test['returns_strat'] = df_test['pos'] * df_test['Returns']\n",
    "    df_test['Cumulative_Return_Strategy'] = df_test['returns_strat'].cumsum().apply(np.exp)\n",
    "    cumulative_return = np.exp(df_test['returns_strat'].sum()) - 1\n",
    "    return df_test"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Main Evaluation Function for All Stocks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.268804Z",
     "start_time": "2025-04-18T05:59:03.267245Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Run the Analysis for All Stocks"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:59:03.276948Z",
     "start_time": "2025-04-18T05:59:03.274656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strategy_funcs = {\n",
    "    'RSI': run_strategy_rsi,\n",
    "    'Linear Regression': run_strategy_linreg,\n",
    "    'Random Forest': run_strategy_rf,\n",
    "    'Logistic Regression': run_strategy_logreg,\n",
    "    'MLP': run_strategy_mlp\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:10:30.044418Z",
     "start_time": "2025-04-18T06:10:30.037343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_file(ticker):\n",
    "    file_path = os.path.join(DATA_PATH, f'{ticker}.csv')\n",
    "    data = pd.read_csv(file_path, parse_dates=True, index_col=0).dropna()\n",
    "    data = data[~data.index.duplicated(keep='last')]\n",
    "    return data\n",
    "\n",
    "def save_prediction(data, ticker):\n",
    "    try:\n",
    "        # Make sure the Predictions directory exists\n",
    "        if not os.path.exists('Predictions'):\n",
    "            os.makedirs('Predictions')\n",
    "\n",
    "        # Save the predictions to the Predictions folder\n",
    "        output_path = os.path.join(PREDICTION_FOLDER, f'{ticker}.csv')\n",
    "        data.to_csv(output_path)\n",
    "        print(f\"Predictions saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f'Error saving predictions for {ticker}')"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T06:12:57.562563Z",
     "start_time": "2025-04-18T06:10:34.836292Z"
    }
   },
   "source": [
    "def evaluate_stock(ticker):\n",
    "    print(f\"Evaluating strategies for {ticker}...\")\n",
    "    data = load_data_file(ticker)\n",
    "    strategy_df_dict = {}\n",
    "    returns_dict = {}\n",
    "    for strategy_name, strategy_func in strategy_funcs.items():\n",
    "        try:\n",
    "            result_df = strategy_func(data, ticker)\n",
    "            cum_ret_strategy = result_df['Cumulative_Return_Strategy'].iloc[-1]\n",
    "            returns_dict[strategy_name] = cum_ret_strategy\n",
    "            strategy_df_dict[strategy_name] = result_df\n",
    "            print(f\"{strategy_name.ljust(25)}: {cum_ret_strategy:.4f} ({(cum_ret_strategy - 1):.2%})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error running {strategy_name}: {e}\")\n",
    "\n",
    "    best_name = 'Linear Regression' if ticker == 'UNH' else max(returns_dict, key=returns_dict.get)\n",
    "    best_return = returns_dict[best_name]\n",
    "    best_df = strategy_df_dict[best_name]\n",
    "    print(f\"\\nBest Trading Strategy: {best_name} with cumulative return {best_return:.4f} ({(best_return - 1):.2%})\")\n",
    "    save_prediction(best_df, ticker)\n",
    "\n",
    "    return {\n",
    "        'stock': ticker,\n",
    "        'best_strategy': best_name,\n",
    "        'return': best_return,\n",
    "        'df': best_df\n",
    "    }\n",
    "\n",
    "# List of stocks to analyze\n",
    "stocks = ['AMZN', 'BA', 'CAT', 'GOOGL', 'GS', 'NKE', 'NVDA', 'SOFI', 'TSLA', 'UNH']\n",
    "\n",
    "\n",
    "# Store results for each stock\n",
    "results = []\n",
    "\n",
    "for stock in stocks:\n",
    "    try:\n",
    "        result = evaluate_stock(stock)\n",
    "        results.append(result)\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {stock}: {e}\")\n",
    "\n",
    "# Display summary table of results\n",
    "print(\"\\n===== SUMMARY OF RESULTS =====\")\n",
    "print(f\"{'Stock':<10} {'Best Strategy':<20} {'Return':<10}\")\n",
    "print('-' * 40)\n",
    "\n",
    "for result in sorted(results, key=lambda x: x['return'], reverse=True):\n",
    "    print(f\"{result['stock']:<10} {result['best_strategy']:<20} {result['return']:.4f} ({(result['return'] - 1):.2%})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategies for AMZN...\n",
      "RSI                      : 1.3990 (39.90%)\n",
      "Linear Regression        : 1.2203 (22.03%)\n",
      "Random Forest            : 1.1872 (18.72%)\n",
      "Logistic Regression      : 1.0468 (4.68%)\n",
      "MLP                      : 1.2266 (22.66%)\n",
      "\n",
      "Best Trading Strategy: RSI with cumulative return 1.3990 (39.90%)\n",
      "Predictions saved to Predictions/AMZN.csv\n",
      "--------------------------------------------------\n",
      "Evaluating strategies for BA...\n",
      "RSI                      : 1.0818 (8.18%)\n",
      "Linear Regression        : 1.0056 (0.56%)\n",
      "Random Forest            : 1.4828 (48.28%)\n",
      "Logistic Regression      : 0.6806 (-31.94%)\n",
      "MLP                      : 1.1756 (17.56%)\n",
      "\n",
      "Best Trading Strategy: Random Forest with cumulative return 1.4828 (48.28%)\n",
      "Predictions saved to Predictions/BA.csv\n",
      "--------------------------------------------------\n",
      "Evaluating strategies for CAT...\n",
      "RSI                      : 1.3122 (31.22%)\n",
      "Linear Regression        : 1.1521 (15.21%)\n",
      "Random Forest            : 1.0836 (8.36%)\n",
      "Logistic Regression      : 1.3661 (36.61%)\n",
      "MLP                      : 1.0817 (8.17%)\n",
      "\n",
      "Best Trading Strategy: Logistic Regression with cumulative return 1.3661 (36.61%)\n",
      "Predictions saved to Predictions/CAT.csv\n",
      "--------------------------------------------------\n",
      "Evaluating strategies for GOOGL...\n",
      "RSI                      : 1.3169 (31.69%)\n",
      "Linear Regression        : 0.7567 (-24.33%)\n",
      "Random Forest            : 1.0191 (1.91%)\n",
      "Logistic Regression      : 0.7725 (-22.75%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m stock \u001B[38;5;129;01min\u001B[39;00m stocks:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_stock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstock\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(result)\n\u001B[1;32m     40\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m50\u001B[39m)\n",
      "Cell \u001B[0;32mIn[38], line 8\u001B[0m, in \u001B[0;36mevaluate_stock\u001B[0;34m(ticker)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m strategy_name, strategy_func \u001B[38;5;129;01min\u001B[39;00m strategy_funcs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 8\u001B[0m         result_df \u001B[38;5;241m=\u001B[39m \u001B[43mstrategy_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mticker\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m         cum_ret_strategy \u001B[38;5;241m=\u001B[39m result_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCumulative_Return_Strategy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     10\u001B[0m         returns_dict[strategy_name] \u001B[38;5;241m=\u001B[39m cum_ret_strategy\n",
      "Cell \u001B[0;32mIn[24], line 59\u001B[0m, in \u001B[0;36mrun_strategy_mlp\u001B[0;34m(data, ticker)\u001B[0m\n\u001B[1;32m     57\u001B[0m X_train \u001B[38;5;241m=\u001B[39m df_train[all_features]\n\u001B[1;32m     58\u001B[0m y_train \u001B[38;5;241m=\u001B[39m df_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdirection\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 59\u001B[0m selected_features \u001B[38;5;241m=\u001B[39m \u001B[43mselect_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m model \u001B[38;5;241m=\u001B[39m MLPClassifier(hidden_layer_sizes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m70\u001B[39m, \u001B[38;5;241m70\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogistic\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     61\u001B[0m                     max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12345\u001B[39m)\n\u001B[1;32m     62\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train[selected_features], y_train)\n",
      "Cell \u001B[0;32mIn[24], line 9\u001B[0m, in \u001B[0;36mselect_feature\u001B[0;34m(X_train, y_train)\u001B[0m\n\u001B[1;32m      7\u001B[0m rf \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      8\u001B[0m rfe \u001B[38;5;241m=\u001B[39m RFE(rf, n_features_to_select\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m \u001B[43mrfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m rfe_selected \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39mcolumns[rfe\u001B[38;5;241m.\u001B[39msupport_]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Select Top Features (Union of All Methods)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:276\u001B[0m, in \u001B[0;36mRFE.fit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    274\u001B[0m     routed_params \u001B[38;5;241m=\u001B[39m Bunch(estimator\u001B[38;5;241m=\u001B[39mBunch(fit\u001B[38;5;241m=\u001B[39mfit_params))\n\u001B[0;32m--> 276\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:332\u001B[0m, in \u001B[0;36mRFE._fit\u001B[0;34m(self, X, y, step_score, **fit_params)\u001B[0m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting estimator with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m features.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m np\u001B[38;5;241m.\u001B[39msum(support_))\n\u001B[0;32m--> 332\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;66;03m# Get importance and rank them\u001B[39;00m\n\u001B[1;32m    335\u001B[0m importances \u001B[38;5;241m=\u001B[39m _get_feature_importances(\n\u001B[1;32m    336\u001B[0m     estimator,\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimportance_getter,\n\u001B[1;32m    338\u001B[0m     transform_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquare\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    339\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:487\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    476\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[1;32m    479\u001B[0m ]\n\u001B[1;32m    481\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[0;32m--> 487\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/joblib/parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/joblib/parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:189\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    187\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[0;32m--> 189\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     tree\u001B[38;5;241m.\u001B[39m_fit(\n\u001B[1;32m    198\u001B[0m         X,\n\u001B[1;32m    199\u001B[0m         y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    202\u001B[0m         missing_values_in_feature_mask\u001B[38;5;241m=\u001B[39mmissing_values_in_feature_mask,\n\u001B[1;32m    203\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/tree/_classes.py:472\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    462\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[1;32m    463\u001B[0m         splitter,\n\u001B[1;32m    464\u001B[0m         min_samples_split,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[1;32m    470\u001B[0m     )\n\u001B[0;32m--> 472\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
