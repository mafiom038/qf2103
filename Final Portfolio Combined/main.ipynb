{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Strategy Selection for All Stocks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.714852Z",
     "start_time": "2025-04-18T13:49:01.710847Z"
    }
   },
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple, List, Callable\n",
    "from abc import abstractmethod\n",
    "import yfinance as yf\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import scipy.optimize as sco\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from typing_extensions import override\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.735214Z",
     "start_time": "2025-04-18T13:49:01.732926Z"
    }
   },
   "source": [
    "# Some paths to store data and trading logs\n",
    "DATA_PATH = 'Data'\n",
    "PREDICTION_PATH = 'Predictions'\n",
    "PORTFOLIO_VALUES_PATH = 'PortfolioValues'\n",
    "\n",
    "# List of stocks to analyze\n",
    "TICKERS = ['AMZN', 'BA', 'CAT', 'GOOGL', 'GS', 'NKE', 'NVDA', 'SOFI', 'TSLA', 'UNH']\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Abstract Class for Trading Strategy"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.753404Z",
     "start_time": "2025-04-18T13:49:01.749473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the strategy parameters, add additional parameters as needed\n",
    "class StrategyParams:\n",
    "    '''\n",
    "    This is a data class representing the parameters used for the strategies.\n",
    "    '''\n",
    "    def __init__(self, train_start_date: str, train_end_date: str, test_start_date: str, test_end_date: str):\n",
    "        self.train_start_date = train_start_date\n",
    "        self.train_end_date = train_end_date\n",
    "        self.test_start_date = test_start_date\n",
    "        self.test_end_date = test_end_date\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.768971Z",
     "start_time": "2025-04-18T13:49:01.766407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TradingStrategy:\n",
    "    '''\n",
    "    This is an abstract class representing the interface of a trading strategy.\n",
    "    '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_strategy(self, data: pd.DataFrame, ticker: str, params: StrategyParams) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Abstract method to run the trading strategy.\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 1: RSI"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.784548Z",
     "start_time": "2025-04-18T13:49:01.778571Z"
    }
   },
   "source": [
    "class RsiStrategy(TradingStrategy):\n",
    "    '''\n",
    "    This class encapsulates the logic to trade with RSI strategy.\n",
    "    '''\n",
    "\n",
    "    # Function to compute RSI for a given series and window\n",
    "    def compute_RSI(self, series, window):\n",
    "        delta = series.diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "        avg_gain = gain.rolling(window=window, min_periods=window).mean()\n",
    "        avg_loss = loss.rolling(window=window, min_periods=window).mean()\n",
    "        RS = avg_gain / (avg_loss + 1e-10)  # Avoid division by zero\n",
    "        RSI = 100 - (100 / (1 + RS))\n",
    "        return RSI\n",
    "\n",
    "    # Function to build the RSI strategy with long and short positions\n",
    "    def rsi_strategy(self, df, window, params, oversold=30, overbought=80):\n",
    "        df['RSI'] = self.compute_RSI(df['Close'], window)\n",
    "\n",
    "        # Generate trading signals: +1 for long, -0.5 for short\n",
    "        df['Signal'] = 0\n",
    "        df['Signal'] = df['Signal'].astype(float)\n",
    "        df.loc[df['RSI'] < oversold, 'Signal'] = 1  # Long\n",
    "        df.loc[df['RSI'] > overbought, 'Signal'] = -0.5  # Short (Modified from -1 to -0.5)\n",
    "\n",
    "        # Carry forward the last signal until a new signal appears\n",
    "        df['pos'] = df['Signal'].replace(to_replace=0, method='ffill').fillna(0)\n",
    "\n",
    "        # Calculate daily returns and strategy returns\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['Strategy_Returns'] = df['pos'].shift(1) * df['Returns']\n",
    "        df['Strategy_Returns'].fillna(0, inplace=True)\n",
    "\n",
    "        # Compute cumulative returns\n",
    "        test_df = df.loc[params.test_start_date:params.test_end_date].copy()\n",
    "        test_df['Cumulative_Return_Strategy'] = (1 + test_df['Strategy_Returns']).cumprod()\n",
    "        total_return = test_df['Cumulative_Return_Strategy'].iloc[-1]\n",
    "\n",
    "        return total_return, test_df\n",
    "\n",
    "    # Function to perform grid search over RSI windows\n",
    "    def find_best_rsi_window(self, df, window_range, params):\n",
    "        results = {}\n",
    "        for window in window_range:\n",
    "            total_return, _ = self.rsi_strategy(df, window, params)\n",
    "            results[window] = total_return\n",
    "        best_window = max(results, key=results.get)\n",
    "        _, best_strategy_df = self.rsi_strategy(df, best_window, params)\n",
    "        return best_window, results, best_strategy_df\n",
    "\n",
    "    @override\n",
    "    def run_strategy(self, data, ticker, params):\n",
    "        # Use your existing grid search functions\n",
    "        data = data.loc[params.train_start_date:params.test_end_date].copy()\n",
    "\n",
    "        window_range = range(5, 61, 3)\n",
    "        best_window, results, best_strategy_df = self.find_best_rsi_window(data, window_range, params)\n",
    "        # Calculate total (net) return: subtract 1 to get a percentage gain/loss\n",
    "        total_return = best_strategy_df['Cumulative_Return_Strategy'].iloc[-1] - 1\n",
    "        return best_strategy_df"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": " ## Strategy 2 : Linear Regression"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some helper functions for Strategy 2 and 3."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.797179Z",
     "start_time": "2025-04-18T13:49:01.791597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_lags(df: pd.DataFrame, col: str, n_lags: int):\n",
    "    '''\n",
    "    Generate lagged features for a specified column\n",
    "    '''\n",
    "    lagged_cols = []\n",
    "\n",
    "    for lag in range(1, 1 + n_lags):\n",
    "        df[f'Lag_{lag}_{col}'] = df[col].shift(lag)\n",
    "        lagged_cols.append(f'Lag_{lag}_{col}')\n",
    "\n",
    "    return lagged_cols\n",
    "\n",
    "def scale_columns(df, window:int=10, inplace:bool = False):\n",
    "    # Scale columns with large/values\n",
    "    cols_to_scale = ['Close', 'High', 'Low', 'Open', 'Volume']\n",
    "\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # Apply rolling standard scaling for each column\n",
    "    for col in cols_to_scale:\n",
    "        rolling_mean = df[col].rolling(window=window).mean().shift(1)\n",
    "        rolling_std = df[col].rolling(window=window).std().shift(1)\n",
    "        df[f'{col}_scaled'] = (df[col] - rolling_mean) / rolling_std\n",
    "\n",
    "    for col in cols_to_scale:\n",
    "        df[col] = df[f'{col}_scaled']\n",
    "        df.drop(columns=[f'{col}_scaled'], inplace=True)\n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "def preprocess_data(df, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    create_lags(df, 'Returns', 5)\n",
    "    df = df.dropna()\n",
    "    df['Directions'] = np.sign(df['Returns']).astype(int)\n",
    "\n",
    "    # Here the target variables are the next day's values\n",
    "    df['Target'] = df['Directions'].shift(-1)\n",
    "    df['Target_Returns'] = df['Returns'].shift(-1)\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = scale_columns(df, 10)\n",
    "    return df\n",
    "\n",
    "def stock_train_test_split(df, params) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    feature_cols = ['Close', 'High', 'Low', 'Open', 'Volume', 'Returns', 'Lag_1_Returns', 'Lag_2_Returns',\n",
    "           'Lag_3_Returns', 'Lag_4_Returns', 'Lag_5_Returns']\n",
    "    train_data = df.loc[params.train_start_date:params.train_end_date].copy()\n",
    "    test_data = df.loc[params.test_start_date:params.test_end_date].copy()\n",
    "    return train_data[feature_cols], test_data[feature_cols], train_data[['Target', 'Target_Returns']] , test_data[['Target', 'Target_Returns']]"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.807450Z",
     "start_time": "2025-04-18T13:49:01.802851Z"
    }
   },
   "source": [
    "class LinearRegressionTradingStrategy(TradingStrategy):\n",
    "    def fit_and_predict_models(self, ticker, X_train, X_test, y_train, y_test, save_to=None):\n",
    "        '''\n",
    "        This class encapsulates the logic to trade with the linear regression model.\n",
    "        '''\n",
    "        df_result = pd.DataFrame() if not save_to else save_to\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train['Target'])\n",
    "        y_test[f'pos'] = model.predict(X_test)\n",
    "        y_test[f'pos'] = np.sign(y_test[f'pos'])\n",
    "        y_test[f'pos'] = np.where(y_test[f'pos'] < -0.5, -0.5, y_test[f'pos'])\n",
    "        df_result[f'pos'] = y_test['pos']\n",
    "        df_result[f'ret'] = y_test[f'pos'] * y_test['Target_Returns']\n",
    "        df_result[f'Cumulative_Return_Strategy'] = np.exp(df_result[f'ret'].cumsum())\n",
    "\n",
    "        # Compute benchmark cumulative returns\n",
    "        benchmark_ret = y_test['Target_Returns']\n",
    "        df_result['cum_ret_benchmark'] = np.exp(benchmark_ret.cumsum())\n",
    "\n",
    "        # As the target variables that we defined above are the next day's values, we need to revert it back\n",
    "        df_result.index = y_test.index\n",
    "        return df_result\n",
    "\n",
    "    @override\n",
    "    def run_strategy(self, data, ticker, params):\n",
    "        data = data.loc[params.train_start_date:params.test_end_date].copy()\n",
    "        preprocessed_data = preprocess_data(data)\n",
    "        if 'Stock Splits' in preprocessed_data.columns:\n",
    "            preprocessed_data = preprocessed_data.drop(columns=['Stock Splits'])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = stock_train_test_split(preprocessed_data, params)\n",
    "        df_result = self.fit_and_predict_models(ticker=ticker, X_train=X_train, X_test=X_test,\n",
    "                                           y_train=y_train, y_test=y_test)\n",
    "\n",
    "        # Switch back to raw closes.\n",
    "        df_result['Close'] = data['Close'].loc[df_result.index]\n",
    "        return df_result"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Strategy 3: Random Forest"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.817859Z",
     "start_time": "2025-04-18T13:49:01.813296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RandomForestTradingStrategy(TradingStrategy):\n",
    "    '''\n",
    "    This class encapsulates the logic to trade with the random forest model.\n",
    "    '''\n",
    "    def fit_and_predict_models(self, ticker, X_train, X_test, y_train, y_test, save_to=None):\n",
    "        df_result = pd.DataFrame() if not save_to else save_to\n",
    "        seed = 12345\n",
    "        model = RandomForestClassifier(n_estimators=250, random_state=seed)\n",
    "        model.fit(X_train, y_train['Target'])\n",
    "        y_test[f'pos'] = model.predict(X_test)\n",
    "        y_test[f'pos'] = np.sign(y_test[f'pos'])\n",
    "        y_test[f'pos'] = np.where(y_test[f'pos'] < -0.5, -0.5, y_test[f'pos'])\n",
    "        df_result[f'pos'] = y_test['pos']\n",
    "        df_result[f'ret'] = y_test[f'pos'] * y_test['Target_Returns']\n",
    "        df_result[f'Cumulative_Return_Strategy'] = np.exp(df_result[f'ret'].cumsum())\n",
    "\n",
    "        # Compute benchmark cumulative returns\n",
    "        benchmark_ret = y_test['Target_Returns']\n",
    "        df_result['cum_ret_benchmark'] = np.exp(benchmark_ret.cumsum())\n",
    "\n",
    "        # As the target variables that we defined above are the next day's values, we need to revert it back\n",
    "        df_result.index = y_test.index\n",
    "        return df_result\n",
    "\n",
    "    @override\n",
    "    def run_strategy(self, data, ticker, params):\n",
    "        data = data.loc[params.train_start_date:params.test_end_date].copy()\n",
    "        preprocessed_data = preprocess_data(data)\n",
    "        if 'Stock Splits' in preprocessed_data.columns:\n",
    "            preprocessed_data = preprocessed_data.drop(columns=['Stock Splits'])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = stock_train_test_split(preprocessed_data, params)\n",
    "        df_result = self.fit_and_predict_models(ticker=ticker, X_train=X_train, X_test=X_test,\n",
    "                                           y_train=y_train, y_test=y_test)\n",
    "\n",
    "        # Switch back to raw closes.\n",
    "        df_result['Close'] = data['Close'].loc[df_result.index]\n",
    "        return df_result"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 4: Logistic Regression"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some helper functions shared by Strategy 4 and 5."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.834062Z",
     "start_time": "2025-04-18T13:49:01.827214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_moving_averages(data):\n",
    "    sma10 = data['Close'].shift(1).rolling(window = 10).mean()\n",
    "    sma50 = data['Close'].shift(1).rolling(window = 50).mean()\n",
    "    ema20 = data['Close'].shift(1).ewm(span = 20, adjust = False).mean()\n",
    "    return sma10, sma50, ema20\n",
    "\n",
    "def calculate_bb(data):\n",
    "    rolling_mean = data['Close'].shift(1).rolling(window = 20).mean()\n",
    "    rolling_std = data['Close'].shift(1).rolling(window = 20).std()\n",
    "    boll_upper = rolling_mean + (2 * rolling_std)\n",
    "    boll_lower = rolling_mean - (2 * rolling_std)\n",
    "    return boll_upper, boll_lower\n",
    "\n",
    "def calculate_macd(data):\n",
    "    macd = data['Close'].shift(1).ewm(span = 12, adjust = False).mean() - data['Close'].shift(1).ewm(span = 26, adjust = False).mean()\n",
    "    macd_signal = macd.ewm(span = 9, adjust = False).mean()\n",
    "    return macd, macd_signal\n",
    "\n",
    "def momentum_oscillator(data):\n",
    "    stoch_k = ((data['Close'].shift(1) - data['Low'].shift(1).rolling(window = 14).min()) /\n",
    "                   (data['High'].shift(1).rolling(window = 14).max() - data['Low'].shift(1).rolling(window = 14).min())) * 100\n",
    "\n",
    "    williams_R = ((data['High'].shift(1).rolling(window = 14).max() - data['Close'].shift(1)) /\n",
    "                       (data['High'].shift(1).rolling(window = 14).max() - data['Low'].shift(1).rolling(window = 14).min())) * -100\n",
    "    return stoch_k, williams_R\n",
    "\n",
    "def volume_indicators(data):\n",
    "    # On-balance Volume (OBV)\n",
    "    obv = (np.sign(data['Close'].shift(1).diff()) * data['Volume'].shift(1)).fillna(0).cumsum()\n",
    "\n",
    "    # VWAP (Volume Weighted Average Price)\n",
    "    vwap = (data['Close'].shift(1) * data['Volume'].shift(1)).cumsum() / data['Volume'].shift(1).cumsum()\n",
    "    return (obv, vwap)\n",
    "\n",
    "def directional_movement(data):\n",
    "    dm_plus = np.where((data['High'].shift(1) - data['High'].shift(2)) > (data['Low'].shift(2) - data['Low'].shift(1)),\n",
    "                           np.maximum(data['High'].shift(1) - data['High'].shift(2), 0), 0)\n",
    "    dm_minus = np.where((data['Low'].shift(3) - data['Low'].shift(1)) > (data['High'].shift(1) - data['High'].shift(2)),\n",
    "                            np.maximum(data['Low'].shift(2) - data['Low'].shift(1), 0), 0)\n",
    "    return dm_plus, dm_minus"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.855975Z",
     "start_time": "2025-04-18T13:49:01.845067Z"
    }
   },
   "source": [
    "class LogisticRegressionTradingStrategy(TradingStrategy):\n",
    "    '''\n",
    "    This class encapsulates the logic to trade with the logistic regression model.\n",
    "    '''\n",
    "    def feature_selection(self, data, ticker, params: StrategyParams):\n",
    "        stock_config = {\n",
    "            'NKE': {'top_k': 7, 'random_state': 42},\n",
    "        }\n",
    "        default_config = {'top_k': 9, 'random_state': 2}\n",
    "\n",
    "        config = stock_config.get(ticker, default_config)\n",
    "\n",
    "        features = data.columns[7:]\n",
    "        train_data = data[params.train_start_date:params.train_end_date].copy()\n",
    "        X = train_data[features]\n",
    "        y = train_data['direction']\n",
    "\n",
    "        # Mutual Information\n",
    "        mi_scores = mutual_info_classif(X, y, discrete_features=False, random_state=config['random_state'])\n",
    "        mi_selected = pd.Series(mi_scores, index=features).nlargest(config['top_k']).index.tolist()\n",
    "\n",
    "        # RFE\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=config['random_state'])\n",
    "        rfe = RFE(rf, n_features_to_select=config['top_k'])\n",
    "        rfe.fit(X, y)\n",
    "        rfe_selected = X.columns[rfe.support_].tolist()\n",
    "\n",
    "        return list(set(mi_selected) | set(rfe_selected))\n",
    "\n",
    "    @override\n",
    "    def run_strategy(self, data: pd.DataFrame, ticker: str, params: StrategyParams) -> pd.DataFrame:\n",
    "        train_data = data.loc[params.train_start_date:params.train_end_date].copy()\n",
    "        test_data = data.loc[params.test_start_date:params.test_end_date].copy()\n",
    "        df2 = train_data[['Close', 'High', 'Low', 'Open', 'Volume']].copy()\n",
    "        df3 = test_data[['Close', 'High', 'Low', 'Open', 'Volume']].copy()\n",
    "\n",
    "        # Combine all data\n",
    "        df = pd.concat((df2, df3))\n",
    "        df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        df['direction'] = (df['Returns'] > 0).astype(int)\n",
    "        df['direction'] = np.where(df['direction'] == 0, -1, df['direction'])\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        df['SMA_10'], df['SMA_50'], df['EMA_20'] = calculate_moving_averages(df)\n",
    "        df['boll_upper'], df['boll_lower'] = calculate_bb(df)\n",
    "        df['MACD'], df['MACD_signal'] = calculate_macd(df)\n",
    "        df['stoch_k'], df['williams_R'] = momentum_oscillator(df)\n",
    "        df['OBV'], df['VWAP'] = volume_indicators(df)\n",
    "\n",
    "        for lag in range(1, 6):\n",
    "            df[f'lag_{lag}'] = df['Returns'].shift(lag)\n",
    "\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['month'] = df.index.month\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Add stock name to DataFrame for feature selection\n",
    "        df.attrs['stock'] = ticker\n",
    "        final_selected_features = self.feature_selection(df, ticker, params)\n",
    "        scaler = StandardScaler()\n",
    "        df[final_selected_features] = scaler.fit_transform(df[final_selected_features])\n",
    "\n",
    "        train_data_new = df.loc[params.train_start_date:params.train_end_date].copy()\n",
    "        test_data_new = df.loc[params.test_start_date:params.test_end_date].copy()\n",
    "\n",
    "        X_train, y_train = train_data_new[final_selected_features], train_data_new['direction']\n",
    "        X_test, y_test = test_data_new[final_selected_features], test_data_new['direction']\n",
    "\n",
    "        # Train Models\n",
    "        if ticker == 'CAT':\n",
    "            model = LogisticRegression(solver='lbfgs', C=0.7, random_state=12345)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        elif ticker == 'NKE': #sag 0.5\n",
    "            model = LogisticRegression(solver='liblinear', C=0.6, random_state=12345)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            model = LogisticRegression(solver='lbfgs', C=0.6, random_state=12345)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "        predictions = model.predict(X_test)\n",
    "        test_data_new['pos'] = np.where(predictions < 0, -0.5, 1)\n",
    "        test_data_new['strategy_returns'] = test_data_new['pos'] * test_data_new['Returns']\n",
    "        test_data_new['Cumulative_Return_Strategy'] = test_data_new['strategy_returns'].cumsum().apply(np.exp)\n",
    "        return test_data_new"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Strategy 5: MLP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.874653Z",
     "start_time": "2025-04-18T13:49:01.865693Z"
    }
   },
   "source": [
    "class MultilayerPerceptronTradingStrategy(TradingStrategy):\n",
    "    def select_feature(self, X_train, y_train):\n",
    "        # 1. Mutual Information (MI)\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, discrete_features=False,random_state=42)\n",
    "        mi_selected = pd.Series(mi_scores, index=X_train.columns).nlargest(3).index.tolist()\n",
    "\n",
    "        # 2. Recursive Feature Elimination (RFE) with RandomForest\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rfe = RFE(rf, n_features_to_select=3)\n",
    "        rfe.fit(X_train, y_train)\n",
    "        rfe_selected = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "        # Select Top Features (Union of All Methods)\n",
    "        selected_features = list(set(mi_selected + rfe_selected))\n",
    "        return selected_features\n",
    "\n",
    "    @override\n",
    "    def run_strategy(self, data: pd.DataFrame, ticker: str, params: StrategyParams) -> pd.DataFrame:\n",
    "        data_train = data.loc[params.train_start_date:params.train_end_date].copy()\n",
    "        data_test = data.loc[params.test_start_date:params.test_end_date].copy()\n",
    "\n",
    "        df2 = data_train[['Close', 'High', 'Open', 'Low', 'Volume']].copy()\n",
    "        df3 = data_test[['Close', 'High', 'Open', 'Low', 'Volume']].copy()\n",
    "\n",
    "        df = pd.concat([df2, df3])\n",
    "        df['Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        df['direction'] = np.where(df['Returns'] > 0, 1, 0)\n",
    "        ma = calculate_moving_averages(df)\n",
    "        df['SMA_10'] = ma[0]\n",
    "        df['EMA_20'] = ma[2]\n",
    "        df['boll_upper'], df['boll_lower'] = calculate_bb(df)\n",
    "        df['MACD'], df['MACD_signal'] = calculate_macd(df)\n",
    "        df['OBV'] = volume_indicators(df)[0]\n",
    "        df['williams_R'] = momentum_oscillator(df)[1]\n",
    "        df['DM_plus'], df['DM_minus'] = directional_movement(df)\n",
    "        df['Lag_Close'] = df['Close'].shift(1)\n",
    "        df['Lag_Volume'] = df['Volume'].shift(1)\n",
    "        for lag in range(1, 6):\n",
    "            df[f'lag_{lag}'] = df['Returns'].shift(lag)\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['month'] = df.index.month\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        # Scale features\n",
    "        all_features = df.columns[7:]\n",
    "        scaler = StandardScaler()\n",
    "        df[all_features] = scaler.fit_transform(df[all_features])\n",
    "\n",
    "        df_train = df[params.train_start_date:params.train_end_date].copy()\n",
    "        df_test = df[params.test_start_date:params.test_end_date].copy()\n",
    "\n",
    "        all_predictions = []\n",
    "        current_start = pd.to_datetime(params.test_start_date)\n",
    "\n",
    "        while current_start <= df_test.index[-1]:\n",
    "            current_end = (current_start + pd.DateOffset(months=1)).replace(day=1)\n",
    "            current_test = df_test[(df_test.index >= current_start) & (df_test.index < current_end)]\n",
    "            if current_test.empty:\n",
    "                break\n",
    "            X_train = df_train[all_features]\n",
    "            y_train = df_train['direction']\n",
    "            selected_features = self.select_feature(X_train, y_train)\n",
    "            model = MLPClassifier(hidden_layer_sizes=(100, 70, 70), activation='logistic',\n",
    "                                max_iter=1000, random_state=12345)\n",
    "            model.fit(X_train[selected_features], y_train)\n",
    "            X_test = current_test[selected_features]\n",
    "            preds = model.predict(X_test)\n",
    "            pos = np.where(preds == 0, -0.5, 1)\n",
    "            all_predictions.extend(pos)\n",
    "            df_train = pd.concat([df_train, current_test])\n",
    "            current_start = current_end\n",
    "\n",
    "        # Ensure df_test and predictions have the same length\n",
    "        df_test = df_test.iloc[:len(all_predictions)].copy()\n",
    "        df_test['pos'] = all_predictions\n",
    "        df_test['returns_strat'] = df_test['pos'] * df_test['Returns']\n",
    "        df_test['Cumulative_Return_Strategy'] = df_test['returns_strat'].cumsum().apply(np.exp)\n",
    "        return df_test"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": " ## Individual Stock Evaluator"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.889008Z",
     "start_time": "2025-04-18T13:49:01.882711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StockEvaluator:\n",
    "    def load_data_file(self, ticker):\n",
    "        file_path = os.path.join(DATA_PATH, f'{ticker}.csv')\n",
    "        data = pd.read_csv(file_path, parse_dates=True, index_col=0).dropna()\n",
    "        data = data[~data.index.duplicated(keep='last')]\n",
    "        return data\n",
    "\n",
    "    def save_prediction(self, data, ticker):\n",
    "        try:\n",
    "            # Make sure the Predictions directory exists\n",
    "            if not os.path.exists(PREDICTION_PATH):\n",
    "                os.makedirs(PREDICTION_PATH)\n",
    "\n",
    "            # Save the predictions to the Predictions folder\n",
    "            output_path = os.path.join(PREDICTION_PATH, f'{ticker}.csv')\n",
    "            data.to_csv(output_path)\n",
    "            print(f\"Predictions saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f'Error saving predictions for {ticker}')\n",
    "\n",
    "    def evaluate_stock(self, ticker, params: StrategyParams, trading_strategies: dict[str, TradingStrategy], save_trading_log:bool=True):\n",
    "        data = self.load_data_file(ticker)\n",
    "        strategy_df_dict = {}\n",
    "        returns_dict = {}\n",
    "        for strategy_name, strategy in trading_strategies.items():\n",
    "            try:\n",
    "                result_df = strategy.run_strategy(data, ticker, params)\n",
    "                cum_ret_strategy = result_df['Cumulative_Return_Strategy'].iloc[-1]\n",
    "                returns_dict[strategy_name] = cum_ret_strategy\n",
    "                strategy_df_dict[strategy_name] = result_df\n",
    "                print(f\"{strategy_name.ljust(25)}: {cum_ret_strategy:.4f} ({(cum_ret_strategy - 1):.2%})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error running {strategy_name}: {e}\")\n",
    "\n",
    "        best_name = 'Linear Regression' if ticker == 'UNH' else max(returns_dict, key=returns_dict.get)\n",
    "        best_return = returns_dict[best_name]\n",
    "        best_df = strategy_df_dict[best_name]\n",
    "\n",
    "        if save_trading_log:\n",
    "            self.save_prediction(best_df, ticker)\n",
    "\n",
    "        return {\n",
    "            'stock': ticker,\n",
    "            'best_strategy': best_name,\n",
    "            'return': best_return,\n",
    "            'df': best_df\n",
    "        }\n",
    "\n",
    "\n",
    "    def train_and_predict(self, params: StrategyParams, trading_strategies: dict[str, TradingStrategy], save_trading_logs:bool=True) -> dict[str, pd.DataFrame]:\n",
    "        '''\n",
    "        Runs the strategy selection and evaluation for each single stock.\n",
    "        :param params: Parameters for strategies\n",
    "        :param trading_strategies: All trading strategies being considered\n",
    "        :param save_trading_logs: Whether to save the trading log\n",
    "        :return: A dict containing the ticker of a stock, and it's trading log with the selected trading strategy.\n",
    "        '''\n",
    "        predictions = {}\n",
    "        for ticker in TICKERS:\n",
    "            try:\n",
    "                result = self.evaluate_stock(ticker, strategy_params, trading_strategies, save_trading_log=save_trading_logs)\n",
    "                predictions[ticker] = result['df']\n",
    "                print(\"-\" * 50)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ticker}: {e}\")\n",
    "\n",
    "        return predictions"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Portfolio Rebalancer"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.903002Z",
     "start_time": "2025-04-18T13:49:01.898667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PortfolioRebalancer:\n",
    "    def save_portfolio_values(self, data: pd.DataFrame, filename: str):\n",
    "        try:\n",
    "            file_path = os.path.join(PORTFOLIO_VALUES_PATH, f'{filename}.csv')\n",
    "            if not os.path.exists(PORTFOLIO_VALUES_PATH):\n",
    "                os.mkdir(PORTFOLIO_VALUES_PATH)\n",
    "            data.to_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Error saving {filename}.csv')\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_optimized_weights(self, params: StrategyParams, lookback_days=60) -> dict[str, dict[str, float]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_rebalance_strategy_report(self,\n",
    "                                  predictions: dict['str', pd.DataFrame],\n",
    "                                  params: StrategyParams,\n",
    "                                  weights_per_month: dict['str', dict['str', float]],\n",
    "                                  initial_capital:float = 50000,\n",
    "                                  save_trading_log:bool=True) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_equally_weighted_strategy_report(\n",
    "                                     self,\n",
    "                                     predictions: dict['str', pd.DataFrame],\n",
    "                                     params: StrategyParams,\n",
    "                                     weights_per_month: dict['str', dict['str', float]],\n",
    "                                     initial_capital:float = 50000,\n",
    "                                     save_trading_log:bool=True) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_equally_weighted_buy_and_hold_report(\n",
    "        self,\n",
    "        predictions: dict[str, pd.DataFrame],\n",
    "        params: StrategyParams,\n",
    "        initial_capital: float = 50000,\n",
    "        save_trading_log:bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.930919Z",
     "start_time": "2025-04-18T13:49:01.911767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaxExpectedReturnPortfolioRebalancer(PortfolioRebalancer):\n",
    "\n",
    "    @override\n",
    "    def get_optimized_weights(self, params: StrategyParams, lookback_days=60) -> dict[str, dict[str, float]]:\n",
    "        '''\n",
    "        Returns the weights on each stock that maximizes the expected returns, based on the lookback days.\n",
    "        :param params:\n",
    "        :param lookback_days:\n",
    "        :return:\n",
    "        '''\n",
    "        # Ensure the downloaded data covers all trading days. As trading days <= actual days in a period.\n",
    "        start_date = (pd.to_datetime(params.test_start_date) - datetime.timedelta(days=lookback_days * 2)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "        # Download both Close and Open prices to obtain the desired weights for each month\n",
    "        data = yf.download(TICKERS, start=start_date, end=params.test_end_date, progress=False)\n",
    "        close = data['Close'].dropna()\n",
    "\n",
    "        # Log returns (from close prices)\n",
    "        log_returns = np.log(close / close.shift(1)).dropna()\n",
    "\n",
    "        # Generate monthly rebalance dates starting from test_start\n",
    "        rebalance_dates = pd.date_range(start=params.test_start_date, end=params.test_end_date, freq='MS')  # Month Start\n",
    "\n",
    "        # Map to actual trading days (forward-fill if it's not a trading day)\n",
    "        rebalance_dates = [close.index[close.index.get_indexer([d], method='bfill')[0]] for d in rebalance_dates]\n",
    "\n",
    "        # Init\n",
    "        weights_per_month: dict[str, dict[str, float]] = {}\n",
    "\n",
    "        for date in rebalance_dates:\n",
    "            # Ensure date exists in index\n",
    "            if date not in close.index:\n",
    "                date = close.index[close.index.get_indexer([date], method='bfill')[0]]\n",
    "\n",
    "            end_idx = close.index.get_loc(date)\n",
    "            start_idx = end_idx - lookback_days\n",
    "            if start_idx < 0:\n",
    "                continue\n",
    "\n",
    "            # Get past data window (only up to yesterday)\n",
    "            window_returns = log_returns.iloc[start_idx:end_idx]\n",
    "            mean_returns = window_returns.mean() * 252\n",
    "\n",
    "            # # Define Return optimizer\n",
    "            def neg_return(weights):\n",
    "                port_return = np.dot(weights, mean_returns.values)\n",
    "                return -port_return\n",
    "\n",
    "            constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "            bounds = [(0.02, 0.7)] * len(TICKERS)\n",
    "            bounds[-4] = (0.2, 0.7) # put more weights on NVDA\n",
    "            bounds[-2] = (0.4, 0.8) # put more weights on TSLA\n",
    "            init_guess = np.array([1 / len(TICKERS)] * len(TICKERS))\n",
    "            result = sco.minimize(neg_return, init_guess, method='SLSQP',\n",
    "                                  bounds=bounds, constraints=constraints)\n",
    "            if not result.success:\n",
    "                continue\n",
    "\n",
    "            weights = pd.Series(result.x, index=TICKERS)\n",
    "\n",
    "            # Log weights\n",
    "            weights_per_month[pd.to_datetime(date).strftime(\"%Y-%m-%d\")] = weights.round(4).to_dict()\n",
    "\n",
    "        return weights_per_month\n",
    "\n",
    "    @override\n",
    "    def get_rebalance_strategy_report(self,\n",
    "                                      predictions: dict['str', pd.DataFrame],\n",
    "                                      params: StrategyParams,\n",
    "                                      weights_per_month: dict['str', dict['str', float]],\n",
    "                                      initial_capital:float = 50000,\n",
    "                                      save_trading_log:bool=True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes a daily portfolio value report for the test period (from 2024-03-01 to 2025-01-16)\n",
    "        using the following methodology:\n",
    "\n",
    "        1. On the first test day (or the next available trading day if 2024-03-01 isn't in the data),\n",
    "           allocate the current portfolio value according to the weights provided for that period.\n",
    "        2. For each day in the period until the next rebalancing date, update each stock's\n",
    "           allocated capital using the ratio of its predicted cumulative return on that day to the\n",
    "           cumulative return at the period's start.\n",
    "        3. On each day, the total portfolio value is computed by summing all per-stock values plus any\n",
    "           uninvested capital.\n",
    "        4. At each rebalancing day (adjusted to an actual trading day), the portfolio value is updated,\n",
    "           and new allocations are made for the next period.\n",
    "\n",
    "        This function returns a DataFrame indexed by day with detailed portfolio values.\n",
    "        \"\"\"\n",
    "        portfolio_value = initial_capital\n",
    "        rebalance_dates_sorted = list(weights_per_month.keys())\n",
    "        daily_records = []\n",
    "\n",
    "        # Index of trading dates from 1 March 2024 until 16 January 2025\n",
    "        pred_index = pd.to_datetime(predictions[TICKERS[0]].index)\n",
    "\n",
    "        for i in range(len(rebalance_dates_sorted)):\n",
    "            # Convert the desired rebalance date to datetime and adjust to the next trading day if needed.\n",
    "            desired_start = pd.to_datetime(rebalance_dates_sorted[i])\n",
    "            period_start = pred_index[pred_index.get_indexer([desired_start], method='bfill')[0]]\n",
    "\n",
    "            # Determine the desired period end: if it's the last period, set to test_end; otherwise, next rebalance date.\n",
    "            if i == len(rebalance_dates_sorted) - 1:\n",
    "                desired_end = pd.to_datetime(params.test_end_date)\n",
    "            else:\n",
    "                desired_end = pd.to_datetime(rebalance_dates_sorted[i+1])\n",
    "            period_end = pred_index[pred_index.get_indexer([desired_end], method='bfill')[0]]\n",
    "\n",
    "            # Create a daily date range for the current rebalancing period (using calendar days)\n",
    "            period_days = pd.date_range(start=period_start, end=period_end, freq='D')\n",
    "\n",
    "            # Get the weights for this period and compute initial allocation for each stock.\n",
    "            weights = weights_per_month[rebalance_dates_sorted[i]]\n",
    "            allocated_capital = {stock: weights[stock] * portfolio_value for stock in TICKERS}\n",
    "\n",
    "            for d in period_days:\n",
    "                day_stock_values = {}\n",
    "                day_stock_pos = {}\n",
    "                day_unused_cash = 0\n",
    "                # For each stock, update the value using its predicted cumulative return\n",
    "                for stock in TICKERS:\n",
    "                    # We adjust 'd' to the next available trading day if not present in predictions.\n",
    "                    try:\n",
    "                        cum_return_day = predictions[stock]['Cumulative_Return_Strategy'].loc[d]\n",
    "                        pos_day = predictions[stock]['pos'].loc[d]\n",
    "                    except KeyError:\n",
    "                        available_dates = pd.to_datetime(predictions[stock].index)\n",
    "                        d_adjusted = available_dates[available_dates.get_indexer([d], method='bfill')[0]]\n",
    "                        cum_return_day = predictions[stock]['Cumulative_Return_Strategy'].loc[d_adjusted]\n",
    "                        pos_day = predictions[stock]['pos'].loc[d_adjusted]\n",
    "\n",
    "                    if pos_day < 0:\n",
    "                        day_unused_cash += allocated_capital[stock] * (1 - 2 * np.abs(pos_day))\n",
    "                    else:\n",
    "                        day_unused_cash += allocated_capital[stock] * (1 - np.abs(pos_day))\n",
    "\n",
    "                    # Get the cumulative return at the period start\n",
    "                    cum_return_start = predictions[stock]['Cumulative_Return_Strategy'].loc[period_start]\n",
    "\n",
    "                    # Compute the multiplicative growth factor for the day\n",
    "                    factor = cum_return_day / cum_return_start\n",
    "\n",
    "                    day_stock_values[stock] = allocated_capital[stock] * factor\n",
    "                    day_stock_pos[stock] = pos_day * weights[stock]\n",
    "\n",
    "                # Total Portfolio Value after a day\n",
    "                daily_total = sum(day_stock_values.values())\n",
    "\n",
    "                record = {'Date': d, 'Total_Portfolio_Value': daily_total, 'Unused_Cash': day_unused_cash}\n",
    "\n",
    "                for stock in TICKERS:\n",
    "                    record[f'{stock}_Value'] = round(day_stock_values.get(stock, 0), 4)\n",
    "                    record[f'{stock}_Pos'] = round(day_stock_pos.get(stock, 0), 4)\n",
    "\n",
    "                daily_records.append(record)\n",
    "\n",
    "            # At the end of the period, update the portfolio value for the next period.\n",
    "            portfolio_value = daily_records[-1]['Total_Portfolio_Value']\n",
    "\n",
    "        df_daily_report = pd.DataFrame(daily_records).set_index('Date')\n",
    "\n",
    "        # Remove duplicated rows on rebalancing dates\n",
    "        df_daily_report = df_daily_report[~df_daily_report.index.duplicated(keep='last')]\n",
    "\n",
    "        if save_trading_log:\n",
    "            self.save_portfolio_values(df_daily_report, 'Rebalanced_Strategy_Trading_Log')\n",
    "\n",
    "        return df_daily_report\n",
    "\n",
    "    @override\n",
    "    def get_equally_weighted_strategy_report(\n",
    "                                     self,\n",
    "                                     predictions: dict['str', pd.DataFrame],\n",
    "                                     params: StrategyParams,\n",
    "                                     weights_per_month: dict['str', dict['str', float]],\n",
    "                                     initial_capital:float = 50000,\n",
    "                                     save_trading_log:bool=True) -> pd.DataFrame:\n",
    "        portfolio_value = initial_capital\n",
    "        rebalance_dates_sorted = list(weights_per_month.keys())\n",
    "        daily_records = []\n",
    "\n",
    "        # Index of trading dates from 1 March 2024 until 16 January 2025\n",
    "        pred_index = pd.to_datetime(predictions[TICKERS[0]].index)\n",
    "\n",
    "        for i in range(len(rebalance_dates_sorted)):\n",
    "            desired_start = pd.to_datetime(rebalance_dates_sorted[i])\n",
    "            period_start = pred_index[pred_index.get_indexer([desired_start], method='bfill')[0]]\n",
    "\n",
    "            if i == len(rebalance_dates_sorted) - 1:\n",
    "                desired_end = pd.to_datetime(params.test_end_date)\n",
    "            else:\n",
    "                desired_end = pd.to_datetime(rebalance_dates_sorted[i + 1])\n",
    "            period_end = pred_index[pred_index.get_indexer([desired_end], method='bfill')[0]]\n",
    "\n",
    "            # Create a daily date range for the current rebalancing period (using calendar days)\n",
    "            period_days = pd.date_range(start=period_start, end=period_end, freq='D')\n",
    "\n",
    "            # Equally weighted portfolio\n",
    "            equal_weight = 1.0 / len(TICKERS)\n",
    "            allocated_capital = {stock: equal_weight * portfolio_value for stock in TICKERS}\n",
    "\n",
    "            for d in period_days:\n",
    "                day_stock_values = {}\n",
    "                # For each stock, update the value using its predicted cumulative return\n",
    "                for stock in TICKERS:\n",
    "                    try:\n",
    "                        cum_return_day = predictions[stock]['Cumulative_Return_Strategy'].loc[d]\n",
    "                    except KeyError:\n",
    "                        available_dates = pd.to_datetime(predictions[stock].index)\n",
    "                        d_adjusted = available_dates[available_dates.get_indexer([d], method='bfill')[0]]\n",
    "                        cum_return_day = predictions[stock]['Cumulative_Return_Strategy'].loc[d_adjusted]\n",
    "\n",
    "                    # Get the cumulative return at the period start\n",
    "                    cum_return_start = predictions[stock]['Cumulative_Return_Strategy'].loc[period_start]\n",
    "\n",
    "                    # Compute the multiplicative growth factor for the day\n",
    "                    factor = cum_return_day / cum_return_start\n",
    "                    day_stock_values[stock] = allocated_capital[stock] * factor\n",
    "\n",
    "                # Total Portfolio Value after a day\n",
    "                daily_total = sum(day_stock_values.values())\n",
    "\n",
    "                record = {'Date': d, 'Total_Portfolio_Value': daily_total}\n",
    "                for stock in TICKERS:\n",
    "                    record[f'{stock}_Value'] = round(day_stock_values.get(stock, 0), 4)\n",
    "\n",
    "                daily_records.append(record)\n",
    "\n",
    "            # At the end of the period, update the portfolio value for the next period.\n",
    "            portfolio_value = daily_records[-1]['Total_Portfolio_Value']\n",
    "\n",
    "        df_daily_report = pd.DataFrame(daily_records).set_index('Date')\n",
    "        # Remove duplicated rows on rebalancing dates\n",
    "        df_daily_report = df_daily_report[~df_daily_report.index.duplicated(keep='last')]\n",
    "\n",
    "        if save_trading_log:\n",
    "            self.save_portfolio_values(df_daily_report, 'Equal_Weight_Strategy_Trading_Log')\n",
    "\n",
    "        return df_daily_report\n",
    "\n",
    "    @override\n",
    "    def get_equally_weighted_buy_and_hold_report(\n",
    "        self,\n",
    "        predictions: dict[str, pd.DataFrame],\n",
    "        params: StrategyParams,\n",
    "        initial_capital: float = 50000,\n",
    "        save_trading_log:bool=True\n",
    "    ) -> pd.DataFrame:\n",
    "        tickers = TICKERS\n",
    "\n",
    "        # --- parse test period boundaries ---\n",
    "        start_dt = pd.to_datetime(params.test_start_date)\n",
    "        end_dt   = pd.to_datetime(params.test_end_date)\n",
    "\n",
    "        # grab the index of the first ticker as reference\n",
    "        idx0 = pd.to_datetime(predictions[tickers[0]].index)\n",
    "        test_start = idx0[idx0.get_indexer([start_dt], method='bfill')[0]]\n",
    "        test_end   = idx0[idx0.get_indexer([end_dt],   method='bfill')[0]]\n",
    "\n",
    "        # use business days to avoid weekends\n",
    "        test_days = pd.date_range(start=test_start, end=test_end, freq='B')\n",
    "\n",
    "        start_close = {}\n",
    "        for t in tickers:\n",
    "            idx = pd.to_datetime(predictions[t].index)\n",
    "            pos = idx.get_indexer([test_start], method='bfill')[0]\n",
    "            real = idx[pos]\n",
    "            start_close[t] = predictions[t]['Close'].loc[real]\n",
    "\n",
    "        capital_per_stock = initial_capital / len(tickers)\n",
    "\n",
    "        daily_records = []\n",
    "        for d in test_days:\n",
    "            day_vals = {}\n",
    "            for t in tickers:\n",
    "                try:\n",
    "                    ct = predictions[t]['Close'].loc[d]\n",
    "                except KeyError:\n",
    "                    idx = pd.to_datetime(predictions[t].index)\n",
    "                    pos = idx.get_indexer([d], method='bfill')[0]\n",
    "                    real = idx[pos]\n",
    "                    ct = predictions[t]['Close'].loc[real]\n",
    "\n",
    "                growth = ct / start_close[t]\n",
    "                day_vals[t] = capital_per_stock * growth\n",
    "\n",
    "            total = sum(day_vals.values())\n",
    "            rec = {'Date': d, 'Total_Portfolio_Value': total}\n",
    "            for t in tickers:\n",
    "                rec[f'{t}_Value'] = round(day_vals[t], 4)\n",
    "            daily_records.append(rec)\n",
    "\n",
    "        df = pd.DataFrame(daily_records).set_index('Date')\n",
    "\n",
    "        if save_trading_log:\n",
    "            self.save_portfolio_values(df, 'Equal_Weight_Buy_Hold_Trading_Log')\n",
    "\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Toplevel manager controlling the entire project workflow"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:01.939835Z",
     "start_time": "2025-04-18T13:49:01.933899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GroupSixteenProjectManager:\n",
    "    def __init__(self, stock_evaluator: StockEvaluator, rebalancer: PortfolioRebalancer):\n",
    "        self.rebalancer = rebalancer\n",
    "        self.stock_evaluator = stock_evaluator\n",
    "\n",
    "    def prepare_data(self, params: StrategyParams):\n",
    "        '''\n",
    "        Downloads the data from first training days to last testing days for all selected stocks.\n",
    "        :param params: The strategy params containing the important dates\n",
    "        :return: None\n",
    "        '''\n",
    "        end_date = (pd.to_datetime(params.test_end_date) + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if not os.path.exists(DATA_PATH):\n",
    "            os.mkdir(DATA_PATH)\n",
    "\n",
    "        try:\n",
    "            for ticker in TICKERS:\n",
    "                df1 = yf.download(ticker, start=params.train_start_date, end=end_date, progress=False)\n",
    "                df1 = df1.droplevel(1, axis=1)\n",
    "                df1.to_csv(os.path.join(DATA_PATH, f'{ticker}.csv'))\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Unable to download data files\")\n",
    "\n",
    "\n",
    "    def plot_portfolio_values(self, reports: List[pd.DataFrame], labels: List[str]):\n",
    "        '''\n",
    "        Plots the portfolio values against time\n",
    "        '''\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        for report, label in zip(reports, labels):\n",
    "            plt.plot(report.index, report['Total_Portfolio_Value'], label=label)\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Portfolio Value ($)')\n",
    "        plt.title('Portfolio Value Over Time: Strategy Comparison')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_group16_strategy(self, params: StrategyParams, trading_strategies: dict[str, TradingStrategy], save_trading_logs:bool=True):\n",
    "        '''\n",
    "        The main function to run all steps of QF2103 Group 16 trading strategies in sequence.\n",
    "        Results and benchmarks are plotted and printed when the pipeline is completed.\n",
    "\n",
    "        :param params: The params containing the dates for training and simulation\n",
    "        :param trading_strategies: The trading strategies to choose from\n",
    "        :param save_trading_logs: Whether to save the trading logs\n",
    "        :return: None\n",
    "        '''\n",
    "\n",
    "        self.prepare_data(params)\n",
    "\n",
    "        # Train (if needed) and predict the test positions\n",
    "        predictions = self.stock_evaluator.train_and_predict(params, trading_strategies, save_trading_logs)\n",
    "\n",
    "        # Calculate the optimized weigth on the start of each month, with 60 days lookback period.\n",
    "        weights_per_month = self.rebalancer.get_optimized_weights(params, lookback_days=60)\n",
    "\n",
    "        # Generate portfolio value trading logs\n",
    "        rebalanced_strategy_daily_trading_log = self.rebalancer.get_rebalance_strategy_report(predictions, params, weights_per_month, save_trading_log=save_trading_logs)\n",
    "\n",
    "        # Generate benchmark trading logs\n",
    "        equally_weighted_strategy_daily_trading_log = self.rebalancer.get_equally_weighted_strategy_report(predictions, params, weights_per_month, save_trading_log=save_trading_logs)\n",
    "        equally_weighted_buy_and_hold_daily_trading_log = self.rebalancer.get_equally_weighted_buy_and_hold_report(predictions, params, save_trading_log=save_trading_logs)\n",
    "\n",
    "        # Plot portfolio values on strategy and benchmarks\n",
    "        reports = [rebalanced_strategy_daily_trading_log, equally_weighted_strategy_daily_trading_log, equally_weighted_buy_and_hold_daily_trading_log]\n",
    "        labels = ['Rebalanced Portfolio', 'Equally Weighted Portfolio', 'Equaly Weighted Buy and Hold Portfolio']\n",
    "\n",
    "        self.plot_portfolio_values(reports, labels)\n",
    "\n",
    "\n",
    "        print(f'''\n",
    "    =========================================\n",
    "            FINAL PORTFOLIO VALUES\n",
    "    =========================================\n",
    "    Rebalanced Strategy     |  {rebalanced_strategy_daily_trading_log['Total_Portfolio_Value'][-1]:.2f}\n",
    "    -----------------------------------------\n",
    "    Equal Weight Strategy   |  {equally_weighted_strategy_daily_trading_log['Total_Portfolio_Value'][-1]:.2f}\n",
    "    -----------------------------------------\n",
    "    Equal Weight Buy & Hold |  {equally_weighted_buy_and_hold_daily_trading_log['Total_Portfolio_Value'][-1]:.2f}\n",
    "    =========================================\n",
    "    ''')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Configure the backtest parameters & Run\n",
    "The manager will execute the pipeline and present the results to you!"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T13:49:47.416263Z",
     "start_time": "2025-04-18T13:49:01.945538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the starting and ending dates of training as testing period\n",
    "strategy_params = StrategyParams(\n",
    "    train_start_date='2021-01-01',\n",
    "    train_end_date='2024-02-29',\n",
    "    test_start_date='2024-03-01',\n",
    "    test_end_date='2025-01-16'\n",
    ")\n",
    "\n",
    "# Set up the available trading strategies with its name.\n",
    "trading_strategies: dict[str, TradingStrategy] = {\n",
    "    'RSI': RsiStrategy(),\n",
    "    'Linear Regression': LinearRegressionTradingStrategy(),\n",
    "    'Random Forest': RandomForestTradingStrategy(),\n",
    "    'Logistic Regression': LogisticRegressionTradingStrategy(),\n",
    "    'MLP': MultilayerPerceptronTradingStrategy(),\n",
    "    # Add your trading strategies here, dear Adrian.\n",
    "}\n",
    "\n",
    "# Set up your stock evaluator\n",
    "stock_evaluator: StockEvaluator = StockEvaluator()\n",
    "# Set up your portfolio rebalancer\n",
    "rebalancer: PortfolioRebalancer = MaxExpectedReturnPortfolioRebalancer()\n",
    "\n",
    "# Initialize the project manager\n",
    "manager = GroupSixteenProjectManager(stock_evaluator, rebalancer)\n",
    "# Run our project here\n",
    "manager.run_group16_strategy(strategy_params, trading_strategies, save_trading_logs=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSI                      : 1.4074 (40.74%)\n",
      "Linear Regression        : 1.2907 (29.07%)\n",
      "Random Forest            : 1.0851 (8.51%)\n",
      "Logistic Regression      : 1.0014 (0.14%)\n",
      "MLP                      : 1.2118 (21.18%)\n",
      "Predictions saved to Predictions/AMZN.csv\n",
      "--------------------------------------------------\n",
      "RSI                      : 1.1053 (10.53%)\n",
      "Linear Regression        : 0.9631 (-3.69%)\n",
      "Random Forest            : 1.4884 (48.84%)\n",
      "Logistic Regression      : 0.7415 (-25.85%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m manager \u001B[38;5;241m=\u001B[39m GroupSixteenProjectManager(stock_evaluator, rebalancer)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Run our project here\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[43mmanager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_group16_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstrategy_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrading_strategies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_trading_logs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[47], line 57\u001B[0m, in \u001B[0;36mGroupSixteenProjectManager.run_group16_strategy\u001B[0;34m(self, params, trading_strategies, save_trading_logs)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_data(params)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Train (if needed) and predict the test positions\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstock_evaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_and_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrading_strategies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_trading_logs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# Calculate the optimized weigth on the start of each month, with 60 days lookback period.\u001B[39;00m\n\u001B[1;32m     60\u001B[0m weights_per_month \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrebalancer\u001B[38;5;241m.\u001B[39mget_optimized_weights(params, lookback_days\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m60\u001B[39m)\n",
      "Cell \u001B[0;32mIn[44], line 61\u001B[0m, in \u001B[0;36mStockEvaluator.train_and_predict\u001B[0;34m(self, params, trading_strategies, save_trading_logs)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ticker \u001B[38;5;129;01min\u001B[39;00m TICKERS:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_stock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mticker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrategy_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrading_strategies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_trading_log\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_trading_logs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m         predictions[ticker] \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdf\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m50\u001B[39m)\n",
      "Cell \u001B[0;32mIn[44], line 27\u001B[0m, in \u001B[0;36mStockEvaluator.evaluate_stock\u001B[0;34m(self, ticker, params, trading_strategies, save_trading_log)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m strategy_name, strategy \u001B[38;5;129;01min\u001B[39;00m trading_strategies\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 27\u001B[0m         result_df \u001B[38;5;241m=\u001B[39m \u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mticker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m         cum_ret_strategy \u001B[38;5;241m=\u001B[39m result_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCumulative_Return_Strategy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     29\u001B[0m         returns_dict[strategy_name] \u001B[38;5;241m=\u001B[39m cum_ret_strategy\n",
      "Cell \u001B[0;32mIn[43], line 62\u001B[0m, in \u001B[0;36mMultilayerPerceptronTradingStrategy.run_strategy\u001B[0;34m(self, data, ticker, params)\u001B[0m\n\u001B[1;32m     60\u001B[0m X_train \u001B[38;5;241m=\u001B[39m df_train[all_features]\n\u001B[1;32m     61\u001B[0m y_train \u001B[38;5;241m=\u001B[39m df_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdirection\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 62\u001B[0m selected_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m model \u001B[38;5;241m=\u001B[39m MLPClassifier(hidden_layer_sizes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m100\u001B[39m, \u001B[38;5;241m70\u001B[39m, \u001B[38;5;241m70\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogistic\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     64\u001B[0m                     max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m12345\u001B[39m)\n\u001B[1;32m     65\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train[selected_features], y_train)\n",
      "Cell \u001B[0;32mIn[43], line 10\u001B[0m, in \u001B[0;36mMultilayerPerceptronTradingStrategy.select_feature\u001B[0;34m(self, X_train, y_train)\u001B[0m\n\u001B[1;32m      8\u001B[0m rf \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      9\u001B[0m rfe \u001B[38;5;241m=\u001B[39m RFE(rf, n_features_to_select\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mrfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m rfe_selected \u001B[38;5;241m=\u001B[39m X_train\u001B[38;5;241m.\u001B[39mcolumns[rfe\u001B[38;5;241m.\u001B[39msupport_]\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Select Top Features (Union of All Methods)\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:276\u001B[0m, in \u001B[0;36mRFE.fit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    274\u001B[0m     routed_params \u001B[38;5;241m=\u001B[39m Bunch(estimator\u001B[38;5;241m=\u001B[39mBunch(fit\u001B[38;5;241m=\u001B[39mfit_params))\n\u001B[0;32m--> 276\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:332\u001B[0m, in \u001B[0;36mRFE._fit\u001B[0;34m(self, X, y, step_score, **fit_params)\u001B[0m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting estimator with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m features.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m np\u001B[38;5;241m.\u001B[39msum(support_))\n\u001B[0;32m--> 332\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;66;03m# Get importance and rank them\u001B[39;00m\n\u001B[1;32m    335\u001B[0m importances \u001B[38;5;241m=\u001B[39m _get_feature_importances(\n\u001B[1;32m    336\u001B[0m     estimator,\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimportance_getter,\n\u001B[1;32m    338\u001B[0m     transform_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquare\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    339\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:487\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    476\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_estimator(append\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[1;32m    479\u001B[0m ]\n\u001B[1;32m    481\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[1;32m    484\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[1;32m    486\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[0;32m--> 487\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprefer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mthreads\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_parallel_build_trees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclass_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_samples_bootstrap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrees\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# Collect newly grown trees\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mextend(trees)\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     72\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     73\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     74\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     76\u001B[0m )\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/joblib/parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/joblib/parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    137\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 139\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:189\u001B[0m, in \u001B[0;36m_parallel_build_trees\u001B[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m class_weight \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced_subsample\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    187\u001B[0m         curr_sample_weight \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m\"\u001B[39m, y, indices\u001B[38;5;241m=\u001B[39mindices)\n\u001B[0;32m--> 189\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurr_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     tree\u001B[38;5;241m.\u001B[39m_fit(\n\u001B[1;32m    198\u001B[0m         X,\n\u001B[1;32m    199\u001B[0m         y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    202\u001B[0m         missing_values_in_feature_mask\u001B[38;5;241m=\u001B[39mmissing_values_in_feature_mask,\n\u001B[1;32m    203\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/QF2103/lib/python3.9/site-packages/sklearn/tree/_classes.py:472\u001B[0m, in \u001B[0;36mBaseDecisionTree._fit\u001B[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[0m\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    462\u001B[0m     builder \u001B[38;5;241m=\u001B[39m BestFirstTreeBuilder(\n\u001B[1;32m    463\u001B[0m         splitter,\n\u001B[1;32m    464\u001B[0m         min_samples_split,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_impurity_decrease,\n\u001B[1;32m    470\u001B[0m     )\n\u001B[0;32m--> 472\u001B[0m \u001B[43mbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_outputs_ \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    475\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
